{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uplift_report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5mOAgeTNy0-o"
      },
      "source": [
        "# Remerge uplift report\n",
        "\n",
        "This notebook allows you to validate remerge provided uplift reporting numbers. To do so it downloads and analyses exported campaign and event data from S3. The campaign data contains all users that remerge marked to be part of an uplift test, the A/B group assignment, the timestamp of marking, conversion events (click, app open or similar) and their cost. The event data reflects the app event stream and includes events, their timestamp and revenue (if any). We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). \n",
        "\n",
        "**Hint**: This notebook can be run in any Jupyter instance with enough space/memory, as a [Google Colab notebook](#Google-Colab-version) or as a standalone Python script. If you are using a copy of this notebook running on Colab or locally you can find the original template on [GitHub: remerge/uplift-report](https://github.com/remerge/uplift-report/blob/master/uplift_report_per_campaign.ipynb)\n",
        "\n",
        "### Notebook configuration\n",
        "\n",
        "For this notebook to work properly several variables in the [Configuration](#Configuration) section need to be be set: `customer`, `audience`, `\n",
        "revenue_event`, `dates` and the AWS credentials. All of these will be provided by your remerge account manager. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j6nnLGh5SUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import remerge uplift-report library\n",
        "import os\n",
        "\n",
        "# if we are in jupyter environment - we have cloned the repo already and `lib` is available\n",
        "# on Colab we need to clone the repo and enable the same loading path through a symlink\n",
        "if not os.path.exists('lib'):\n",
        "    !git clone --branch internal https://github.com/remerge/uplift-report.git\n",
        "    !ln -s uplift-report/lib\n",
        "    \n",
        "    !pip install lib/\n",
        "    \n",
        "    # Since we could have upgraded some dependencies, that require restart of the kernel (specifically `pandas`),\n",
        "    # it is safer to perform this restart now\n",
        "    os.kill(os.getpid(), 9)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kyYz6TCny0-q"
      },
      "source": [
        "## Import packages\n",
        "\n",
        "This notebook/script needs our Uplift Report helper library, as long as several other dependencies it brings with it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9hjp-yR6MsK",
        "colab_type": "text"
      },
      "source": [
        "## Load helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jucQTKF86PaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from lib.helpers import Helpers\n",
        "\n",
        "from IPython.display import display  # so we can run this as script as well"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Novl387nrno"
      },
      "source": [
        "## Version\n",
        "Version of the analysis script corresponding to the methodology version in the whitepaper (Major + Minor version represent the whitepaper version, revision represents changes and fixes of the uplift report script)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2vm5Z9UoHe6",
        "colab": {}
      },
      "source": [
        "display(Helpers.version())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8u6Q76fCy0-u"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set the customer name, audience and access credentials for the S3 bucket and path. Furthermore the event for which we want to evaluate the uplift needs to be set `revenue_event`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmeaSE-2HjAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "customer = ''\n",
        "audiences = ['']\n",
        "# date range for the report\n",
        "dates = pd.date_range(start='2020-01-20',end='2020-01-20')\n",
        "# AWS credentials\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
        "\n",
        "# Configure the reporting output: \n",
        "\n",
        "# named groups that aggregate several campaigns\n",
        "groups = {}\n",
        "\n",
        "# show uplift results per campaign:\n",
        "per_campaign_results = False\n",
        "\n",
        "# base statistical calculations on unique converters instead of conversions\n",
        "use_converters_for_significance = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wFgBi4jvsVTn",
        "colab": {}
      },
      "source": [
        "# Instantiate & configure the helpers\n",
        "#\n",
        "# Hint: Press Atl + / or Tab to see docstring with paramerets descriptions in Google Colab\n",
        "helpers = Helpers(\n",
        "    customer=customer,\n",
        "    audiences=audiences,\n",
        "    revenue_event='purchase',\n",
        "    dates=dates,\n",
        "    attribution_dates=dates,\n",
        "    groups=groups,\n",
        "    use_converters_for_significance=use_converters_for_significance,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om5t8fQJMQfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import s3fs\n",
        "import pandas as pd\n",
        "\n",
        "from lib.helpers import _S3CachedFile\n",
        "\n",
        "def read_king_s3_csv(customer, audience, date):\n",
        "    now = datetime.now()\n",
        "\n",
        "    file_path_template = 's3://remerge-customers/{0}/uplift_data/{1}/attributions/{2}.csv'\n",
        "\n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "\n",
        "    filename = file_path_template.format(\n",
        "        customer,\n",
        "        audience,\n",
        "        date_str,\n",
        "    )\n",
        "\n",
        "    print('start loading CSV for', audience, date)\n",
        "    print('filename', filename)\n",
        "\n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    fs.connect_timeout = 10  # defaults to 5\n",
        "    fs.read_timeout = 30  # defaults to 15\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    if not fs.exists(path=filename):\n",
        "        print('WARNING: no CSV file at for: ', audience, date, ', skipping the file: ', filename)\n",
        "        return df\n",
        "\n",
        "    read_csv_kwargs = {'chunksize': 10 ** 3}\n",
        "\n",
        "    with _S3CachedFile(fs, filename) as s3_file:\n",
        "        print('starting processing CSV for', date.strftime('%d.%m.%Y'))\n",
        "        for chunk in pd.read_csv(s3_file.local_path, escapechar='\\\\', low_memory=False, **read_csv_kwargs):\n",
        "            df = pd.concat([df, chunk], ignore_index=True, verify_integrity=True)\n",
        "    \n",
        "    print('finished processing CSV for', date.strftime('%d.%m.%Y'), 'took', datetime.now() - now)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rFZvNNgUqGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST = \"test\"\n",
        "CONTROL = \"control\"\n",
        "\n",
        "def king_uplift(\n",
        "    marks_and_spend_df, \n",
        "    attributions_df, \n",
        "    index_name=\"total\", \n",
        "    m_hypothesis=1, \n",
        "    use_converters_for_significance=False,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    # Uplift Calculation\n",
        "    We calculate the incremental revenue and the iROAS in line with the\n",
        "    [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). Afterwards we run\n",
        "    a [chi squared test](https://en.wikipedia.org/wiki/Chi-squared_test) on the results to test for significance of\n",
        "    the results, comparing conversion to per group uniques.\n",
        "    \"\"\"\n",
        "    \n",
        "    # calculate group sizes\n",
        "    test_group_size = attributions_df[attributions_df['group_name'] == TEST]['user_id'].nunique()\n",
        "    if test_group_size == 0:\n",
        "        log(\"WARNING: No users marked as test for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "\n",
        "    control_group_size = attributions_df[attributions_df['group_name'] == CONTROL]['user_id'].nunique()\n",
        "    if control_group_size == 0:\n",
        "        log(\"WARNING: No users marked as control for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "\n",
        "    # join marks and revenue events\n",
        "    grouped_revenue = attributions_df.groupby(by='group_name')\n",
        "\n",
        "    # init all KPIs with 0s first:\n",
        "    test_revenue_dollars = 0\n",
        "    test_conversions = 0\n",
        "    test_converters = 0\n",
        "\n",
        "    control_revenue_dollars = 0\n",
        "    control_conversions = 0\n",
        "    control_converters = 0\n",
        "\n",
        "    # we might not have any events for a certain group in the time-period,\n",
        "    if TEST in grouped_revenue.groups:\n",
        "        test_revenue_df = grouped_revenue.get_group(TEST)\n",
        "        test_revenue_dollars = test_revenue_df['revenue'].sum()\n",
        "        test_conversions = test_revenue_df['user_id'].count()\n",
        "        test_converters = test_revenue_df['user_id'].nunique()\n",
        "\n",
        "    if CONTROL in grouped_revenue.groups:\n",
        "        control_revenue_df = grouped_revenue.get_group(CONTROL)\n",
        "        control_revenue_dollars = control_revenue_df['revenue'].sum()\n",
        "        # control_conversions = control_revenue_df['partner_event'].count()\n",
        "        # as we filtered by revenue event and dropped the column we can just use\n",
        "        control_conversions = control_revenue_df['user_id'].count()\n",
        "        control_converters = control_revenue_df['user_id'].nunique()\n",
        "\n",
        "    # calculate KPIs\n",
        "    test_revenue = test_revenue_dollars\n",
        "    control_revenue = control_revenue_dollars\n",
        "\n",
        "    ratio = float(test_group_size) / float(control_group_size)\n",
        "    scaled_control_conversions = float(control_conversions) * ratio\n",
        "    scaled_control_revenue_dollars = float(control_revenue_dollars) * ratio\n",
        "    incremental_conversions = test_conversions - scaled_control_conversions\n",
        "    incremental_revenue_dollars = test_revenue_dollars - scaled_control_revenue_dollars\n",
        "    incremental_revenue = incremental_revenue_dollars\n",
        "    incremental_converters = test_converters - control_converters * ratio\n",
        "\n",
        "    # calculate the ad spend\n",
        "    ad_spend = marks_and_spend_df[(marks_and_spend_df.event_type == 'buying_conversion') & (marks_and_spend_df.ab_test_group == True)]['cost'].sum() / 10 ** 6\n",
        "    \n",
        "    iroas = incremental_revenue / ad_spend\n",
        "    icpa = ad_spend / incremental_conversions\n",
        "    cost_per_incremental_converter = ad_spend / incremental_converters\n",
        "\n",
        "    rev_per_conversion_test = 0\n",
        "    rev_per_conversion_control = 0\n",
        "    if test_conversions > 0:\n",
        "        rev_per_conversion_test = test_revenue / test_conversions\n",
        "    if control_conversions > 0:\n",
        "        rev_per_conversion_control = control_revenue / control_conversions\n",
        "\n",
        "    test_cvr = test_conversions / test_group_size\n",
        "    control_cvr = control_conversions / control_group_size\n",
        "\n",
        "    uplift = 0\n",
        "    if control_cvr > 0:\n",
        "        uplift = test_cvr / control_cvr - 1\n",
        "\n",
        "    # calculate statistical significance\n",
        "    control_successes, test_successes = control_conversions, test_conversions\n",
        "    if use_converters_for_significance or max(test_cvr, control_cvr) > 1.0:\n",
        "        control_successes, test_successes = control_converters, test_converters\n",
        "    chi_df = pd.DataFrame({\n",
        "        \"conversions\": [control_successes, test_successes],\n",
        "        \"total\": [control_group_size, test_group_size]\n",
        "    }, index=[CONTROL, TEST])\n",
        "    # CHI square calculation will fail with insufficient data\n",
        "    # Fallback to no significance\n",
        "    try:\n",
        "        chi, p, _, _ = scipy.stats.chi2_contingency(\n",
        "            pd.concat([chi_df.total - chi_df.conversions, chi_df.conversions], axis=1), correction=False)\n",
        "    except:\n",
        "        chi, p = 0, 1.0\n",
        "\n",
        "    # bonferroni correction with equal weights - if we have multiple hypothesis:\n",
        "    # https://en.wikipedia.org/wiki/Bonferroni_correction\n",
        "    significant = p < 0.05 / m_hypothesis\n",
        "\n",
        "    dataframe_dict = {\n",
        "        \"ad spend\": ad_spend,\n",
        "        \"total revenue\": test_revenue + control_revenue,\n",
        "        \"test group size\": test_group_size,\n",
        "        \"test conversions\": test_conversions,\n",
        "        \"test converters\": test_converters,\n",
        "        \"test revenue\": test_revenue,\n",
        "        \"control group size\": control_group_size,\n",
        "        \"control conversions\": control_conversions,\n",
        "        \"control_converters\": control_converters,\n",
        "        \"control revenue\": control_revenue,\n",
        "        \"ratio test/control\": ratio,\n",
        "        \"control conversions (scaled)\": scaled_control_conversions,\n",
        "        \"control revenue (scaled)\": scaled_control_revenue_dollars,\n",
        "        \"incremental conversions\": incremental_conversions,\n",
        "        \"incremental converters\": incremental_converters,\n",
        "        \"incremental revenue\": incremental_revenue,\n",
        "        \"rev/conversions test\": rev_per_conversion_test,\n",
        "        \"rev/conversions control\": rev_per_conversion_control,\n",
        "        \"test CVR\": test_cvr,\n",
        "        \"control CVR\": control_cvr,\n",
        "        \"CVR Uplift\": uplift,\n",
        "        \"iROAS\": iroas,\n",
        "        \"cost per incr. converter\": cost_per_incremental_converter,\n",
        "        \"iCPA\": icpa,\n",
        "        \"chi^2\": chi,\n",
        "        \"p-value\": p,\n",
        "        \"significant\": significant\n",
        "    }\n",
        "\n",
        "    # show results as a dataframe\n",
        "    return pd.DataFrame(\n",
        "        dataframe_dict,\n",
        "        index=[index_name],\n",
        "    ).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eSixTLyiy0_A"
      },
      "source": [
        "## Load CSV data from S3\n",
        "\n",
        "Load mark, spend and event data from S3. \n",
        "\n",
        "### IMPORTANT\n",
        "\n",
        "**The event data is usually quite large (several GB) so this operation might take several minutes or hours to complete, depending on the size and connection.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjWaWZS-y0_B",
        "colab": {}
      },
      "source": [
        "marks_and_spend_df = helpers.load_marks_and_spend_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFg_-_EW5TR1",
        "colab": {}
      },
      "source": [
        "attributions_df = pd.concat(\n",
        "    [read_king_s3_csv(\n",
        "        customer=customer,\n",
        "        audience=audience,\n",
        "        date=date,\n",
        "    ) for audience in audiences for date in dates],\n",
        "    ignore_index=True,\n",
        "    verify_integrity=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ec_qFUaVy0_I"
      },
      "source": [
        "Print some statistics of the loaded data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0Ih6SSuy0_J",
        "colab": {}
      },
      "source": [
        "marks_and_spend_df.info(memory_usage='deep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoU_cW07y0_M",
        "colab": {}
      },
      "source": [
        "attributions_df.info(memory_usage='deep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEHEFE11TJd_",
        "colab_type": "text"
      },
      "source": [
        "### Remove users in both control and test group\n",
        "\n",
        "Very rarly due to timing issue users could be marked for both control and test group. Those are filtered out in this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7kRRqHvTYJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "marks_and_spend_df = helpers.remove_users_marked_as_control_and_test(marks_and_spend_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tKdBRvkxL8Aa"
      },
      "source": [
        "### Calculate and display uplift report for the data set as a whole\n",
        "\n",
        "This takes the whole data set and calculates uplift KPIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SjX4uu6tTpK2",
        "colab": {}
      },
      "source": [
        "report_df = king_uplift(\n",
        "    marks_and_spend_df=marks_and_spend_df, \n",
        "    attributions_df=attributions_df,\n",
        "    use_converters_for_significance=use_converters_for_significance,\n",
        "    )\n",
        "\n",
        "# if there are groups filter the events against the per campaign groups and generate report\n",
        "if report_df is not None and groups:\n",
        "    for name, campaigns in groups.items():\n",
        "        group_df = marks_and_spend_df[marks_and_spend_df.campaign_id.isin(campaigns)]\n",
        "        report_df[name] = king_uplift(\n",
        "            marks_and_spend_df=group_df,\n",
        "            attributions_df=attributions_df,\n",
        "            index_name=name,\n",
        "            m_hypothesis=len(groups))\n",
        "\n",
        "if report_df is not None and per_campaign_results:\n",
        "    campaigns = marks_and_spend_df['campaign_id'].unique()\n",
        "    for campaign in campaigns:\n",
        "        name = \"c_{0}\".format(campaign)\n",
        "        campaign_df = marks_and_spend_df[marks_and_spend_df.campaign_id == campaign]\n",
        "        report_df[name] = king_uplift(\n",
        "            marks_and_spend_df=campaign_df,\n",
        "            attributions_df=attributions_df,\n",
        "            index_name=name,\n",
        "            m_hypothesis=len(campaigns),\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SdFSmL3u8Pe4"
      },
      "source": [
        "## Uplift Results\n",
        "\n",
        "You can configure the ouput by using variables in the 'Configuration' section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWQXKYXB8YO2",
        "colab": {}
      },
      "source": [
        "# set formatting options\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2UZOvmlaXqO",
        "colab": {}
      },
      "source": [
        "display(report_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6VA_k2BobaZS"
      },
      "source": [
        "### CSV Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zztt4QGiSQju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = dates[0]\n",
        "end = dates[-1]\n",
        "\n",
        "helpers.export_csv(df=report, file_name='{}_{}-{}.csv'.format(customer, start, end))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cTdswZU8f2h",
        "colab_type": "text"
      },
      "source": [
        "# Evolution through time\n",
        "Show the evolution of our KPIs through time iterating through `dates` and compiling a report for each day from `start` to `day`.\n",
        "\n",
        "The report can be customized to use a different `report column` (a group column name or campaign) instead of `'total'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaPBFngq8edk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# time development analysis\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = [20, 20]\n",
        "\n",
        "# evolution configuration\n",
        "# which sub column to use for reporting, default='total'\n",
        "report_column = 'total'\n",
        "\n",
        "# which columns of the report to plot graphically\n",
        "plot_columns = ['test group size', 'test CVR', 'control CVR', 'CVR Uplift', 'p-value']\n",
        "\n",
        "def filter_df(df, from_date, to_date):\n",
        "    return df[(df['ts'] >= from_date.timestamp()) & (df['ts'] <= (to_date + timedelta(days=1)).timestamp())]\n",
        "\n",
        "def analyze_evolution(report_column, plot_columns):\n",
        "    start = dates[0]\n",
        "    reports_df = pd.DataFrame()\n",
        "\n",
        "    for end in dates:\n",
        "        marks = filter_df(marks_and_spend_df, start, end)\n",
        "        attr = filter_df(attributions_df, start, end)\n",
        "        report = helpers.uplift_report(marks_and_spend_df=marks, attributions_df=attr)\n",
        "        # if we miss a data file the report will be empty\n",
        "        if report is None:\n",
        "            continue\n",
        "        report = report[[report_column]]\n",
        "        report = report.transpose()\n",
        "        report['date'] = end\n",
        "        report = report.set_index('date')\n",
        "        reports_df = reports_df.append(report)  \n",
        "\n",
        "    #display full reports per day as table\n",
        "    display(reports_df)\n",
        "\n",
        "    # plot the selected columns\n",
        "    plot_df = reports_df[plot_columns]\n",
        "    plot_df.plot.line(subplots=True, grid=True)\n",
        "    return reports_df\n",
        "\n",
        "evolution_report_df = analyze_evolution(report_column, plot_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQFiPIm-Sx4t",
        "colab_type": "text"
      },
      "source": [
        "### Export daily evolution to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K-T6quwwbObO",
        "colab": {}
      },
      "source": [
        "# helpers.export_csv(df=evolution_report_df, file_name='daily_evolution_{}_{}-{}.csv'.format(customer, start, end)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrBuO21ujIJb",
        "colab_type": "text"
      },
      "source": [
        "## Export to reports overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReonxIuujIik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helpers.export_to_overview(report)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}