{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joom of uplift report v2 - kpi improvements.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "eSixTLyiy0_A"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgen/uplift-report/blob/master/uplift_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5mOAgeTNy0-o"
      },
      "cell_type": "markdown",
      "source": [
        "# remerge uplift report\n",
        "\n",
        "This notebook allows you to validate remerge provided uplift reporting numbers. To do so it downloads and analyses exported campaign and event data from S3. The campaign data contains all users that remerge marked to be part of an uplift test, the A/B group assignment, the timestamp of marking, conversion events (click, app open or similar) and their cost. The event data reflects the app event stream and includes events, their timestamp and revenue (if any). We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). \n",
        "\n",
        "**Hint**: This notebook can be run in any Jupyter instance with enough space/memory, as a [Google Colab notebook](#Google-Colab-version) or as a standalone Python script. If you are using a copy of this notebook running on Colab or locally you can find the original template on [GitHub: remerge/uplift-report](https://github.com/remerge/uplift-report/blob/master/uplift_report_per_campaign.ipynb)\n",
        "\n",
        "### Notebook configuration\n",
        "\n",
        "For this notebook to work properly several variables in the [Configuration](#Configuration) section need to be be set: `customer`, `audience`, `\n",
        "revenue_event`, `dates` and the AWS credentials. All of these will be provided by your remerge account manager. \n",
        "\n",
        "\n",
        "### Verification\n",
        "\n",
        "To verify that the group split is random and has no bias, user events / attributes before the campaign start can be compared and checked for an equal distribution in test and control group. For example the user age distribution, the user activity distribution or the average spend per user  should be the same in both groups pre campaign.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QTxBfrT4_Vvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "OYbsRBPTaCZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Colab support\n",
        "\n",
        "This notebook can be run inside Google Colab. Due to size limitations it cointains several optimizations like removing unused fields from the input files and caching files. Furthermore it installs missing dependencies and restarts the kernel. **Because pandas is upgraded the kernel needs to be restarted once per fresh instance. Just run the cell again after restart** "
      ]
    },
    {
      "metadata": {
        "id": "QoJAOpL0aEIT",
        "colab_type": "code",
        "outputId": "ed0b39f8-3414-4108-f902-4792c5c4798f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  !pip install pyarrow\n",
        "  !pip install gspread-pandas\n",
        "  import pandas as pdt\n",
        "  if pdt.__version__ != '0.23.4':\n",
        "    # upgrading pandas requires a restart of the kernel\n",
        "    # (we need an up to date pandas because we write to S3 for caching)\n",
        "    # we kill it and let it auto restart (only needed once per fresh instance)\n",
        "    !pip install pandas==0.23.4\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarrow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/37/eb9aefcd6a041dffb4db6729daea2a91a01a1bf9815e02a3d35281348a85/pyarrow-0.12.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.4MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.14.6)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.11.0)\n",
            "Installing collected packages: pyarrow\n",
            "Successfully installed pyarrow-0.12.1\n",
            "Collecting gspread-pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/30/3aec481d852baf982be7622333b182fcd019523de8c33472751844eae832/gspread-pandas-1.2.1.tar.gz\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (4.1.3)\n",
            "Requirement already satisfied: gspread>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (3.0.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (0.22.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (4.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (0.16.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->gspread-pandas) (0.11.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->gspread-pandas) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->gspread-pandas) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->gspread-pandas) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->gspread-pandas) (0.2.4)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from gspread>=3.0.0->gspread-pandas) (2.18.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->gspread-pandas) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->gspread-pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->gspread-pandas) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread-pandas) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread-pandas) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread-pandas) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread>=3.0.0->gspread-pandas) (3.0.4)\n",
            "Building wheels for collected packages: gspread-pandas\n",
            "  Building wheel for gspread-pandas (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/08/89/5a/62a9cb12b4e9bb9127fa4fd57cebaf9a21dd8e83a349d7a4d3\n",
            "Successfully built gspread-pandas\n",
            "Installing collected packages: gspread-pandas\n",
            "Successfully installed gspread-pandas-1.2.1\n",
            "Collecting pandas==0.23.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.9MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (1.14.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas==0.23.4) (1.11.0)\n",
            "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas\n",
            "  Found existing installation: pandas 0.22.0\n",
            "    Uninstalling pandas-0.22.0:\n",
            "      Successfully uninstalled pandas-0.22.0\n",
            "Successfully installed pandas-0.23.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kyYz6TCny0-q"
      },
      "cell_type": "markdown",
      "source": [
        "## Import needed packages\n",
        "\n",
        "This notebook/script needs pandas and scipy for analysis and boto to access data store on S3.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GWizAQT3y0-r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import scipy\n",
        "import scipy.stats \n",
        "import s3fs\n",
        "from IPython.display import display # so we can run this as script as well\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8u6Q76fCy0-u"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Set the customer name, audience and access credentials for the S3 bucket and path. Furthermore the event for which we want to evaluate the uplift needs to be set `revenue_event`."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aRd9FvoUy0-v",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# configure path and revenue event \n",
        "customer = ''\n",
        "audiences = ['']\n",
        "revenue_event = 'purchase'\n",
        "\n",
        "# date range for the report\n",
        "dates = pd.date_range(start='2019-01-01',end='2019-01-01')\n",
        "\n",
        "# AWS credentials\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ''\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = ''\n",
        "\n",
        "# Configure the reporting output: \n",
        "\n",
        "# named groups that aggregate several campaigns\n",
        "groups = {}\n",
        "\n",
        "# show uplift results per campaign:\n",
        "per_campaign_results = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7XnJeThPiSye"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper\n",
        "Define a few helper functions to load and cache data."
      ]
    },
    {
      "metadata": {
        "id": "DjkoB3CXJI9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def path(audience):\n",
        "  return \"s3://remerge-customers/{0}/uplift_data/{1}\".format(customer,audience)\n",
        "\n",
        "def filter_attributions_df(df):\n",
        "  return df[df.partner_event == revenue_event]\n",
        "  \n",
        "# helper to download CSV files, convert to DF and print time needed\n",
        "# caches files locally and on S3 to be reused\n",
        "def read_csv(audience, source, date, chunk_filter_fn=None, chunk_size=10**6):\n",
        "    now = datetime.now()\n",
        "    \n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "    \n",
        "    filename = '{0}/{1}/{2}.csv.gz'.format(path(audience), source, date_str)\n",
        "    \n",
        "    # local cache\n",
        "    cache_dir = 'cache/{0}/{1}'.format(audience, source)\n",
        "    cache_filename = '{0}/{1}.parquet'.format(cache_dir, date_str)\n",
        "    \n",
        "    # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "    s3_cache_filename = '{0}/{1}/cache/{2}.parquet'.format(path(audience), \n",
        "                                                           source, date_str)\n",
        "    \n",
        "    if source == 'attributions':\n",
        "      cache_filename = '{0}/{1}-{2}.parquet'.format(cache_dir, date_str, \n",
        "                                                    revenue_event)\n",
        "      \n",
        "      # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "      s3_cache_filename = '{0}/{1}/cache/{2}-{3}.parquet' \\\n",
        "        .format(path(audience), source, date_str, revenue_event)\n",
        "\n",
        "    if os.path.exists(cache_filename):\n",
        "        print(now, 'loading from', cache_filename)\n",
        "        return pd.read_parquet(cache_filename, engine='pyarrow')\n",
        "    \n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    \n",
        "    if fs.exists(path=s3_cache_filename):\n",
        "      print(now, 'loading from S3 cache', s3_cache_filename)\n",
        "      return pd.read_parquet(s3_cache_filename, engine='pyarrow')\n",
        "    \n",
        "    print(now, 'start loading CSV for', audience, source, date)\n",
        "    \n",
        "    read_csv_kwargs = {'chunksize': chunk_size}\n",
        "    \n",
        "    if source == 'attributions':\n",
        "      # Only read the columns that are going to be used from attribution\n",
        "      read_csv_kwargs['usecols'] = ['ts', 'user_id', 'partner_event', \n",
        "                                    'revenue_eur', 'ab_test_group']\n",
        "      \n",
        "    df = pd.DataFrame()\n",
        "    for chunk in pd.read_csv(filename, escapechar='\\\\', low_memory=False,\n",
        "                             **read_csv_kwargs):\n",
        "      if chunk_filter_fn:\n",
        "        filtered_chunk = chunk_filter_fn(chunk)\n",
        "      else:\n",
        "        filtered_chunk = chunk\n",
        "      \n",
        "      df = pd.concat([df, filtered_chunk], \n",
        "                     ignore_index = True, verify_integrity=True)\n",
        "    \n",
        "    print(datetime.now(), 'finished loading CSV for', date.strftime('%d.%m.%Y'),\n",
        "          'took', datetime.now()-now)\n",
        "    \n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "        \n",
        "    df.to_parquet(cache_filename, engine='pyarrow')\n",
        "    \n",
        "    # write it to the S3 cache folder as well\n",
        "    print(datetime.now(), 'caching as parquet', s3_cache_filename)\n",
        "    \n",
        "    df.to_parquet(s3_cache_filename, engine='pyarrow')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eSixTLyiy0_A"
      },
      "cell_type": "markdown",
      "source": [
        "## Load CSV data from S3\n",
        "\n",
        "Load mark, spend and event data from S3. \n",
        "\n",
        "### IMPORTANT\n",
        "\n",
        "**The event data is usually quite large (several GB) so this operation might take several minutes or hours to complete, depending on the size and connection.**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PjWaWZS-y0_B",
        "outputId": "9978b724-ad3b-4ae6-9cdb-cc43f648b0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "bids_df = pd.concat([read_csv(audience,'marks_and_spend',date) for audience in audiences for date in dates], ignore_index = True, verify_integrity=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-21 16:49:05.233484 loading from S3 cache s3://remerge-customers/joom/uplift_data/932_Joom_iOS/marks_and_spend/cache/20190128.parquet\n",
            "2019-03-21 16:49:05.633867 loading from S3 cache s3://remerge-customers/joom/uplift_data/931_Joom_android/marks_and_spend/cache/20190128.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kFg_-_EW5TR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "11af0a8d-cca2-4b8a-e4ec-6029024c9920"
      },
      "cell_type": "code",
      "source": [
        "attributions_df = pd.concat([read_csv(audience, 'attributions', date, filter_attributions_df) for audience in audiences for date in dates], ignore_index = True, verify_integrity=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-21 16:49:11.523659 start loading CSV for 932_Joom_iOS attributions 2019-01-28 00:00:00\n",
            "2019-03-21 16:51:52.899797 finished loading CSV for 28.01.2019 took 0:02:41.376173\n",
            "2019-03-21 16:51:52.971058 caching as parquet s3://remerge-customers/joom/uplift_data/932_Joom_iOS/attributions/cache/20190128-cart_purchased.parquet\n",
            "2019-03-21 16:51:53.288748 loading from cache/931_Joom_android/attributions/20190128-cart_purchased.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ec_qFUaVy0_I"
      },
      "cell_type": "markdown",
      "source": [
        "Print some statistics of the loaded data sets."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N0Ih6SSuy0_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "5efb1226-ac45-42fb-e21c-88d0492db63d"
      },
      "cell_type": "code",
      "source": [
        "bids_df.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 98304 entries, 0 to 98303\n",
            "Data columns (total 9 columns):\n",
            "ts               98304 non-null object\n",
            "event_type       98304 non-null object\n",
            "ab_test_group    98304 non-null object\n",
            "user_id          98303 non-null object\n",
            "campaign_id      98304 non-null int64\n",
            "cost_currency    16188 non-null object\n",
            "cost             16188 non-null float64\n",
            "cost_eur         16188 non-null float64\n",
            "campaign_name    98304 non-null object\n",
            "dtypes: float64(2), int64(1), object(6)\n",
            "memory usage: 6.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EoU_cW07y0_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "6d521214-b1a8-4adf-ecd6-a735390fb271"
      },
      "cell_type": "code",
      "source": [
        "attributions_df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 286490 entries, 0 to 286489\n",
            "Data columns (total 8 columns):\n",
            "ab_test_group       120721 non-null object\n",
            "partner             51404 non-null object\n",
            "partner_event       286490 non-null object\n",
            "revenue             49639 non-null float64\n",
            "revenue_currency    51404 non-null object\n",
            "revenue_eur         274780 non-null float64\n",
            "ts                  286490 non-null object\n",
            "user_id             286490 non-null object\n",
            "dtypes: float64(2), object(6)\n",
            "memory usage: 17.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XinLEFZcy0_V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set formatting options\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mFwmhMrJy0_z"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove invalid users\n",
        "\n",
        "Due to a race condition during marking we need to filter out users that are marked as *control* and *test*. In rare cases we see the same user on different servers in the same second, and unknowingly of each other marked him differently. This was fixed in the latest version of the remerge plattform but we need to filter old data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5bnOSTKly0_1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# users that are in both groups due to racy bids are invalid\n",
        "# we need to filter them out\n",
        "groups_per_user = bids_df.groupby('user_id')['ab_test_group'].nunique()\n",
        "invalid_users = groups_per_user[groups_per_user > 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4XVQNpr9y0_7"
      },
      "cell_type": "markdown",
      "source": [
        "## Define functions to prepare data frames\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IPPvDwWIy1AC"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate the cost of advertising give a dataframe. Remerge tracks monetary values in micro currency units. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZEISdQDny1AC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ad_spend(df):\n",
        "  ad_spend_micros = df[df.event_type == 'buying_conversion']['cost_eur'].sum()\n",
        "  return ad_spend_micros / 10**6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgJuPnbAJu3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataframe created by `marked`  will contain all mark events (without the invalid marks). Remerge marks users per campaign.  If a user was marked once for an audience he will have the same group allocation for consecutive marks (different campaigns) unless manually reset on audience level.  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a6XtI0Iqy0_8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def marked(df):\n",
        "  mark_df = df[df.event_type == 'mark']\n",
        "  mark_df = mark_df[~mark_df['user_id'].isin(invalid_users.index)]\n",
        "  sorted_mark_df = mark_df.sort_values('ts')\n",
        "  depuplicated_mark_df = sorted_mark_df.drop_duplicates(['user_id'])\n",
        "  return depuplicated_mark_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EM9SbRf2y1AG"
      },
      "cell_type": "markdown",
      "source": [
        "`revenue` creates a dataframe that contains all relevant revenue events."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ONjCFbzIy1AH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def revenue(df):\n",
        "  revenue_df = df[pd.notnull(df['revenue_eur'])]\n",
        "  return revenue_df[revenue_df.partner_event == revenue_event]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ph6eaF4Ny1Ad"
      },
      "cell_type": "markdown",
      "source": [
        "`merge` joins the marked users with the revenue events and excludes any revenue event that happend before the user was marked."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3mReofc4y1Ad",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def merge(mark_df,revenue_df):\n",
        "  merged_df = pd.merge(revenue_df, mark_df, on='user_id')\n",
        "  return merged_df[merged_df.ts_x > merged_df.ts_y]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "armi-3kmy1Ag"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate uplift kpis\n",
        "\n",
        "We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). Afterwards we run a [chi squared test](https://en.wikipedia.org/wiki/Chi-squared_test) on the results to test for significance of the results, comparing conversion to per group uniques."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V1vKf_u5y1Ag",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def uplift(ad_spend,mark_df,revenue_df,index_name):\n",
        "  # group marked users by their ab_test_group\n",
        "  grouped = mark_df.groupby(by='ab_test_group')\n",
        "  control_df = grouped.get_group('control')\n",
        "  test_df = grouped.get_group('test')\n",
        "  \n",
        "  # join marks and revenue events\n",
        "  merged_df = merge(mark_df,revenue_df)\n",
        "  grouped_revenue = merged_df.groupby(by='ab_test_group_y')\n",
        "  \n",
        "  # init all KPIs with 0s first:\n",
        "  test_revenue_micros = 0\n",
        "  test_conversions = 0\n",
        "  test_converters = 0\n",
        "  \n",
        "  control_revenue_micros = 0\n",
        "  control_conversions = 0\n",
        "  control_converters = 0\n",
        "\n",
        "  # we might not have any events for a certain group in the time-period,\n",
        "  if 'test' in grouped_revenue.groups:\n",
        "    test_revenue_df = grouped_revenue.get_group('test')\n",
        "    test_revenue_micros = test_revenue_df['revenue_eur'].sum()  \n",
        "    test_conversions = test_revenue_df['revenue_eur'].count()  \n",
        "    test_converters = test_revenue_df[test_revenue_df.partner_event == revenue_event]['user_id'].nunique()\n",
        "  \n",
        "  if 'control' in grouped_revenue.groups:\n",
        "    control_revenue_df = grouped_revenue.get_group('control')\n",
        "    control_revenue_micros = control_revenue_df['revenue_eur'].sum()  \n",
        "    control_conversions = control_revenue_df['revenue_eur'].count()\n",
        "    control_converters = control_revenue_df[control_revenue_df.partner_event == revenue_event]['user_id'].nunique()\n",
        "\n",
        "    \n",
        "  # calculate KPIs\n",
        "  test_group_size = test_df['user_id'].nunique()\n",
        "  test_revenue = test_revenue_micros / 10**6\n",
        "  control_group_size = control_df['user_id'].nunique()\n",
        "\n",
        "  control_revenue = control_revenue_micros / 10**6\n",
        "\n",
        "\n",
        "  ratio = float(test_group_size) / float(control_group_size)\n",
        "  scaled_control_conversions = float(control_conversions) * ratio\n",
        "  scaled_control_revenue_micros = float(control_revenue_micros) * ratio\n",
        "  incremental_conversions = test_conversions - scaled_control_conversions\n",
        "  incremental_revenue_micros = test_revenue_micros - scaled_control_revenue_micros\n",
        "  incremental_revenue = incremental_revenue_micros / 10**6\n",
        "  iroas = incremental_revenue / ad_spend\n",
        "  \n",
        "  rev_per_conversion_test = 0\n",
        "  rev_per_conversion_control = 0\n",
        "  if test_conversions > 0:\n",
        "    rev_per_conversion_test = test_revenue / test_conversions\n",
        "  if control_conversions > 0:\n",
        "    rev_per_conversion_control = control_revenue / control_conversions\n",
        "\n",
        "  \n",
        "  test_cvr = test_conversions / test_group_size\n",
        "  control_cvr = control_conversions / control_group_size\n",
        "  \n",
        "  uplift = 0\n",
        "  if control_cvr > 0:\n",
        "    uplift = test_cvr/control_cvr - 1\n",
        "  \n",
        "  \n",
        "  control_successes, test_successes = control_conversions, test_conversions\n",
        "  if max(test_cvr, control_cvr) > 1.0:\n",
        "    control_successes, test_successes = control_converters, test_converters \n",
        "  chi_df = pd.DataFrame({\n",
        "    \"conversions\": [control_successes, test_successes],\n",
        "    \"total\": [control_group_size, test_group_size]\n",
        "    }, index=['control', 'test'])\n",
        "  # CHI square calculation will fail with insufficient data\n",
        "  # Fallback to no significance\n",
        "  try: \n",
        "    chi,p,*_ = scipy.stats.chi2_contingency(pd.concat([chi_df.total - chi_df.conversions, chi_df.conversions], axis=1), correction=False)\n",
        "  except:\n",
        "    chi,p = 0,1.0\n",
        "  \n",
        "  # show results as a dataframe\n",
        "  return pd.DataFrame({\n",
        "    \"ad spend\": ad_spend,\n",
        "    \"total revenue\": test_revenue + control_revenue,\n",
        "    \"test group size\": test_group_size,\n",
        "    \"test conversions\": test_conversions,\n",
        "    \"test converters\": test_converters,\n",
        "    \"test revenue\": test_revenue,\n",
        "\n",
        "    \"control group size\": control_group_size,\n",
        "    \"control conversions\": control_conversions,\n",
        "    \"control_converters\": control_converters,\n",
        "    \"control revenue\": control_revenue,\n",
        "    \"ratio test/control\": ratio,\n",
        "    \"control conversions (scaled)\": scaled_control_conversions,\n",
        "    \"control revenue (scaled)\": scaled_control_revenue_micros / 10**6,\n",
        "    \"incremental conversions\": incremental_conversions,\n",
        "    \"incremental revenue\": incremental_revenue,\n",
        "    \"rev/conversions test\": rev_per_conversion_test,\n",
        "    \"rev/conversions control\": rev_per_conversion_control,\n",
        "    \"test CVR\": test_cvr,\n",
        "    \"control CVR\": control_cvr,\n",
        "    \"CVR Uplift\": uplift,\n",
        "    \"iROAS\": iroas,\n",
        "    \"chi^2\": chi,\n",
        "    \"p-value\": p,\n",
        "    \"significant\": p<0.05},index=[index_name]).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKdBRvkxL8Aa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate and display uplift report for the data set as a whole\n",
        "\n",
        "This takes the whole data set and calculates uplift KPIs."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2y91jjPVy1Ai",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calculate the total result:\n",
        "revenue_df = revenue(attributions_df)\n",
        "mark_df = marked(bids_df)\n",
        "results_df = uplift(ad_spend(bids_df),mark_df,revenue_df,\"total\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lcw0S2p2MHwu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate uplift report per group (if configured)\n",
        "\n",
        "Sometimes it makes sense to look at groups of similar campaigns. If the `groups`  dictionary contains group names as keys and a list of campaign ids as values per key, this function will compile a per group report. "
      ]
    },
    {
      "metadata": {
        "id": "w-OaxnWLGXTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# if there are groups filter the events against the per campaign groups and generate report\n",
        "if len(groups) > 0:\n",
        "  per_group_df = None\n",
        "  for name, campaigns in groups.items():\n",
        "    group_marks_df = bids_df[bids_df.campaign_id.isin(campaigns)]\n",
        "    results_df[name] = uplift(ad_spend(group_marks_df),marked(group_marks_df),revenue_df,name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbZZDsqcMPrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate uplift report per campaign\n",
        "\n",
        "Sometimes it makes sense to look at the uplift report per campaign. Each campaign usually reflects one segement of users. To do that we iterate over all campaigns in the current dataset."
      ]
    },
    {
      "metadata": {
        "id": "bOfbElnCEHhP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if per_campaign_results:\n",
        "  for campaign in bids_df['campaign_id'].unique():\n",
        "    name = \"c_{0}\".format(campaign)\n",
        "    df = bids_df[bids_df.campaign_id == campaign]\n",
        "    results_df[name] = uplift(ad_spend(df),marked(df),revenue_df,name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdFSmL3u8Pe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Uplift Results\n",
        "\n",
        "You can configure the ouput by using variables in the 'Configuration' section"
      ]
    },
    {
      "metadata": {
        "id": "GWQXKYXB8YO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "b08739db-73f9-46ae-9bc4-64b04359e421"
      },
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ad spend</th>\n",
              "      <td>1136.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total revenue</th>\n",
              "      <td>15366.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test group size</th>\n",
              "      <td>71641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test conversions</th>\n",
              "      <td>3101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test converters</th>\n",
              "      <td>2136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test revenue</th>\n",
              "      <td>13876.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control group size</th>\n",
              "      <td>7849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversions</th>\n",
              "      <td>375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control_converters</th>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue</th>\n",
              "      <td>1489.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ratio test/control</th>\n",
              "      <td>9.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversions (scaled)</th>\n",
              "      <td>3422.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue (scaled)</th>\n",
              "      <td>13597.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental conversions</th>\n",
              "      <td>-321.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental revenue</th>\n",
              "      <td>278.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions test</th>\n",
              "      <td>4.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions control</th>\n",
              "      <td>3.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test CVR</th>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control CVR</th>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CVR Uplift</th>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iROAS</th>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chi^2</th>\n",
              "      <td>3.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p-value</th>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>significant</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                total\n",
              "ad spend                      1136.38\n",
              "total revenue                15366.43\n",
              "test group size                 71641\n",
              "test conversions                 3101\n",
              "test converters                  2136\n",
              "test revenue                 13876.64\n",
              "control group size               7849\n",
              "control conversions               375\n",
              "control_converters                242\n",
              "control revenue               1489.79\n",
              "ratio test/control               9.13\n",
              "control conversions (scaled)  3422.78\n",
              "control revenue (scaled)     13597.88\n",
              "incremental conversions       -321.78\n",
              "incremental revenue            278.76\n",
              "rev/conversions test             4.47\n",
              "rev/conversions control          3.97\n",
              "test CVR                         0.04\n",
              "control CVR                      0.05\n",
              "CVR Uplift                      -0.09\n",
              "iROAS                            0.25\n",
              "chi^2                            3.41\n",
              "p-value                          0.06\n",
              "significant                     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "yYzuV0T38aT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}