{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uplift_report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remerge/uplift-report/blob/remove-invalid-users-low-memory/uplift_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5mOAgeTNy0-o"
      },
      "source": [
        "# remerge uplift report\n",
        "\n",
        "This notebook allows you to validate remerge provided uplift reporting numbers. To do so it downloads and analyses exported campaign and event data from S3. The campaign data contains all users that remerge marked to be part of an uplift test, the A/B group assignment, the timestamp of marking, conversion events (click, app open or similar) and their cost. The event data reflects the app event stream and includes events, their timestamp and revenue (if any). We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). \n",
        "\n",
        "**Hint**: This notebook can be run in any Jupyter instance with enough space/memory, as a [Google Colab notebook](#Google-Colab-version) or as a standalone Python script. If you are using a copy of this notebook running on Colab or locally you can find the original template on [GitHub: remerge/uplift-report](https://github.com/remerge/uplift-report/blob/master/uplift_report_per_campaign.ipynb)\n",
        "\n",
        "### Notebook configuration\n",
        "\n",
        "For this notebook to work properly several variables in the [Configuration](#Configuration) section need to be be set: `customer`, `audience`, `\n",
        "revenue_event`, `dates` and the AWS credentials. All of these will be provided by your remerge account manager. \n",
        "\n",
        "\n",
        "### Verification\n",
        "\n",
        "To verify that the group split is random and has no bias, user events / attributes before the campaign start can be compared and checked for an equal distribution in test and control group. For example the user age distribution, the user activity distribution or the average spend per user  should be the same in both groups pre campaign.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYbsRBPTaCZW",
        "colab_type": "text"
      },
      "source": [
        "## Google Colab support\n",
        "\n",
        "This notebook can be run inside Google Colab. Due to size limitations it cointains several optimizations like removing unused fields from the input files and caching files. Furthermore it installs missing dependencies and restarts the kernel. **If pandas was upgraded the kernel needs to be restarted once per fresh instance. Just run the cell again after restart** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoJAOpL0aEIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install pyarrow\n",
        "    !pip install xxhash\n",
        "    !pip install partd\n",
        "    \n",
        "    import pandas as pdt\n",
        "    if pdt.__version__ < '0.23.4':\n",
        "        # upgrading pandas requires a restart of the kernel\n",
        "        # (we need an up to date pandas because we write to S3 for caching)\n",
        "        # we kill it and let it auto restart (only needed once per fresh instance)\n",
        "        !pip install pandas==0.23.4\n",
        "        \n",
        "        import os\n",
        "        os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kyYz6TCny0-q"
      },
      "source": [
        "## Import needed packages\n",
        "\n",
        "This notebook/script needs pandas and scipy for analysis and boto to access data store on S3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWizAQT3y0-r",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xxhash\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import s3fs\n",
        "from IPython.display import display  # so we can run this as script as well"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8u6Q76fCy0-u"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set the customer name, audience and access credentials for the S3 bucket and path. Furthermore the event for which we want to evaluate the uplift needs to be set `revenue_event`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFgBi4jvsVTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure path and revenue event \n",
        "customer = ''\n",
        "audiences = ['']\n",
        "revenue_event = 'purchase'\n",
        "\n",
        "# date range for the report\n",
        "dates = pd.date_range(start='2019-01-01',end='2019-01-01')\n",
        "\n",
        "# AWS credentials\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ''\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = ''\n",
        "\n",
        "# Configure the reporting output: \n",
        "\n",
        "# named groups that aggregate several campaigns\n",
        "groups = {}\n",
        "\n",
        "# show uplift results per campaign:\n",
        "per_campaign_results = False\n",
        "\n",
        "# base statistical calculations on unique converters instead of conversions\n",
        "use_converters_for_significance = False\n",
        "\n",
        "# enable deduplication heuristic for appsflyer\n",
        "use_deduplication = False\n",
        "\n",
        "# name of the folder to use for caching files\n",
        "cache_folder = \"cache-new\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7XnJeThPiSye"
      },
      "source": [
        "## Data loading helpers\n",
        "Define a few helper functions to load and cache data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjkoB3CXJI9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constants for groups\n",
        "TEST = True\n",
        "CONTROL = False\n",
        "\n",
        "# use more memory efficient types for ts,user_id and ab_test_group\n",
        "def improve_types(df):\n",
        "    df['ts'] = pd.to_datetime(df['ts'])\n",
        "    df['ts'] = (df['ts'].astype('int64') / 1e9).astype('int32')\n",
        "    df['user_id'] = df['user_id'].apply(xxhash.xxh64_intdigest).astype('int64')\n",
        "    df['ab_test_group'] = df['ab_test_group'].transform(lambda g: g == 'test')\n",
        "    return df\n",
        "\n",
        "\n",
        "def path(audience):\n",
        "    return \"s3://remerge-customers/{0}/uplift_data/{1}\".format(customer, audience)\n",
        "\n",
        "\n",
        "# only keep rows where the event is a revenue event and drop the partner_event\n",
        "# column afterwards \n",
        "def extract_revenue_events(df):\n",
        "    df = df[df.partner_event == revenue_event]\n",
        "    return df.drop(columns=['partner_event'])\n",
        "    \n",
        "# parquet save and load helper\n",
        "def to_parquet(df, filename):\n",
        "    df.to_parquet(filename, engine='pyarrow')\n",
        "\n",
        "\n",
        "# a little \"hack\" to convert old file on the fly\n",
        "def from_parquet_corrected(filename, s3_filename, fs, columns):\n",
        "    df = from_parquet(filename)\n",
        "    update_cache = False\n",
        "    if columns:\n",
        "      to_drop = list(set(df.columns.values) - set(columns))\n",
        "      if to_drop:\n",
        "        df = df.drop(columns=to_drop)\n",
        "        update_cache = True\n",
        "        \n",
        "    # remove events without a user id\n",
        "    if df['user_id'].dtype == 'object':\n",
        "      if df[df['user_id'].isnull()].empty == False or df[df['user_id'].str.len() != 36].empty == False:          \n",
        "        df = df[df['user_id'].str.len() == 36]\n",
        "        update_cache = True\n",
        "\n",
        "   \n",
        "    if df['user_id'].dtype != 'int64':\n",
        "      df = improve_types(df)\n",
        "      update_cache = True\n",
        "    \n",
        "    if update_cache:\n",
        "      print(datetime.now(), 'rewritting cached file with correct types (local and S3)', filename, s3_filename)\n",
        "      to_parquet(df, filename)\n",
        "      fs.put(filename, s3_filename)\n",
        "    \n",
        "    return df\n",
        "\n",
        "    \n",
        "def from_parquet(filename):\n",
        "    return pd.read_parquet(filename, engine='pyarrow')\n",
        "\n",
        "\n",
        "# helper to download CSV files, convert to DF and print time needed\n",
        "# caches files locally and on S3 to be reused\n",
        "def read_csv(audience, source, date, columns=None, chunk_filter_fn=None, chunk_size=10 ** 6):\n",
        "    now = datetime.now()\n",
        "\n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "    \n",
        "    filename = '{0}/{1}/{2}.csv.gz'.format(path(audience), source, date_str)\n",
        "\n",
        "    # local cache\n",
        "    cache_dir = '{0}/{1}/{2}'.format(cache_folder, audience, source)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "    \n",
        "    cache_filename = '{0}/{1}.parquet'.format(cache_dir, date_str)\n",
        "\n",
        "    # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "    s3_cache_filename = '{0}/{1}/{2}/{3}.parquet'.format(path(audience),\n",
        "                                                           source, cache_folder, date_str)\n",
        "\n",
        "    if source == 'attributions':\n",
        "        cache_filename = '{0}/{1}-{2}.parquet'.format(cache_dir, date_str,\n",
        "                                                      revenue_event)\n",
        "\n",
        "        # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "        s3_cache_filename = '{0}/{1}/{2}/{3}-{4}.parquet' \\\n",
        "            .format(path(audience), source, cache_folder, date_str, revenue_event)\n",
        "\n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    fs.connect_timeout = 10  # defaults to 5\n",
        "    fs.read_timeout = 30  # defaults to 15 \n",
        "\n",
        "    if os.path.exists(cache_filename):\n",
        "        print(now, 'loading from', cache_filename)\n",
        "        return from_parquet_corrected(cache_filename, s3_cache_filename, fs, columns)\n",
        "\n",
        "    \n",
        "    if fs.exists(path=s3_cache_filename):\n",
        "        print(now, 'loading from S3 cache', s3_cache_filename)\n",
        "        \n",
        "        # Download the file to local cache first to avoid timeouts during the load.\n",
        "        # This way, if they happen, restart will be using local copies first.\n",
        "        fs.get(s3_cache_filename, cache_filename)\n",
        "        \n",
        "        print(now, 'stored S3 cache file to local drive, loading', cache_filename)\n",
        "        \n",
        "        return from_parquet_corrected(cache_filename, s3_cache_filename, fs, columns)\n",
        "\n",
        "    print(now, 'start loading CSV for', audience, source, date)\n",
        "\n",
        "    read_csv_kwargs = {'chunksize': chunk_size}\n",
        "    if columns:\n",
        "        read_csv_kwargs['usecols'] = columns\n",
        "        \n",
        "    df = pd.DataFrame()\n",
        "    \n",
        "    if not fs.exists(path=filename):\n",
        "       print(now, 'WARNING: no CSV file at for: ', audience, source, date, ', skipping the file: ', filename)\n",
        "       return df\n",
        "      \n",
        "    for chunk in pd.read_csv(filename, escapechar='\\\\', low_memory=False,\n",
        "                             **read_csv_kwargs):\n",
        "        if chunk_filter_fn:\n",
        "            filtered_chunk = chunk_filter_fn(chunk)\n",
        "        else:\n",
        "            filtered_chunk = chunk\n",
        "        \n",
        "        # we are not interessted in events that do not have a group\n",
        "        filtered_chunk = filtered_chunk[filtered_chunk['ab_test_group'].isin(['test','control'])]\n",
        "        # remove events without a user id\n",
        "        filtered_chunk = filtered_chunk[filtered_chunk['user_id'].str.len() == 36]\n",
        "\n",
        "        # change the types to save alot of memory\n",
        "        filtered_chunk = improve_types(filtered_chunk)\n",
        "\n",
        "        df = pd.concat([df, filtered_chunk],\n",
        "                       ignore_index=True, verify_integrity=True)\n",
        "\n",
        "    print(datetime.now(), 'finished loading CSV for', date.strftime('%d.%m.%Y'),\n",
        "          'took', datetime.now() - now)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    print(datetime.now(), 'caching local as parquet', cache_filename)\n",
        "    to_parquet(df, cache_filename)\n",
        "\n",
        "    # write it to the S3 cache folder as well\n",
        "    print(datetime.now(), 'caching on S3 as parquet', s3_cache_filename)\n",
        "    to_parquet(df, s3_cache_filename)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjWaWZS-y0_B",
        "colab": {}
      },
      "source": [
        "bid_columns = ['ts', 'user_id', 'ab_test_group', 'campaign_id','cost_eur','event_type']\n",
        "bids_df = pd.concat([read_csv(audience, 'marks_and_spend', date, columns=bid_columns) for audience in audiences for date in dates],\n",
        "                    ignore_index=True, verify_integrity=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eSixTLyiy0_A"
      },
      "source": [
        "## Load CSV data from S3\n",
        "\n",
        "Load mark, spend and event data from S3. \n",
        "\n",
        "### IMPORTANT\n",
        "\n",
        "**The event data is usually quite large (several GB) so this operation might take several minutes or hours to complete, depending on the size and connection.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFg_-_EW5TR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attribution_columns = ['ts', 'user_id', 'partner_event', 'revenue_eur', 'ab_test_group']\n",
        "attributions_df = pd.concat(\n",
        "    [read_csv(audience, 'attributions', date, attribution_columns, extract_revenue_events ) for audience in audiences for date in dates],\n",
        "    ignore_index=True, verify_integrity=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ec_qFUaVy0_I"
      },
      "source": [
        "Print some statistics of the loaded data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0Ih6SSuy0_J",
        "colab": {}
      },
      "source": [
        "# bids_df.info(memory_usage='deep')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoU_cW07y0_M",
        "colab": {}
      },
      "source": [
        "# attributions_df.info(memory_usage='deep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G1u_v7pD1ZS",
        "colab_type": "text"
      },
      "source": [
        "## DataFrame Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEISdQDny1AC",
        "colab": {}
      },
      "source": [
        "def calculate_ad_spend(df):\n",
        "    ad_spend_micros = df[df.event_type == 'buying_conversion']['cost_eur'].sum()\n",
        "    \n",
        "    return ad_spend_micros / 10 ** 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgJuPnbAJu3w",
        "colab_type": "text"
      },
      "source": [
        "The dataframe created by `marked`  will contain all mark events (without the invalid marks). Remerge marks users per campaign.  If a user was marked once for an audience he will have the same group allocation for consecutive marks (different campaigns) unless manually reset on audience level.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a6XtI0Iqy0_8",
        "colab": {}
      },
      "source": [
        "def marked(df):\n",
        "    mark_df = df[df.event_type == 'mark']\n",
        "    \n",
        "    # we dont need the event_type anymore (to save memory)\n",
        "    mark_df = mark_df.drop(columns=['event_type'])\n",
        "    \n",
        "    mark_df = mark_df[~mark_df['user_id'].isin(double_marked_users.index)]\n",
        "       \n",
        "    sorted_mark_df = mark_df.sort_values('ts')\n",
        "    \n",
        "    depuplicated_mark_df = sorted_mark_df.drop_duplicates(['user_id'])\n",
        "    \n",
        "    return depuplicated_mark_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ph6eaF4Ny1Ad"
      },
      "source": [
        "`merge` joins the marked users with the revenue events and excludes any revenue event that happend before the user was marked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3mReofc4y1Ad",
        "colab": {}
      },
      "source": [
        "def merge(mark_df, attributions_df):\n",
        "    merged_df = pd.merge(attributions_df, mark_df, on='user_id')\n",
        "    \n",
        "    return merged_df[merged_df.ts_x > merged_df.ts_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1KwS86NELkQ",
        "colab_type": "text"
      },
      "source": [
        "## Clean the data\n",
        "Due to some inconsistencies in the measurement we need to clean the data before analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFECm5p45wpM",
        "colab_type": "text"
      },
      "source": [
        "### Remove duplicated events coming from AppsFlyer\n",
        "\n",
        "AppsFlyer is sending us two revenue events if they attribute the event to us. One of the events they send us does not contain attribution information and the other one does. Sadly, it is not possible for us to distingish correctly if an event is a duplicate or if the user actually triggered two events with nearly the same information. Therefore we rely on a heuristic. We consider an event a duplicate if the user and revenue are equal and the events are less than a minute apart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA6vaITL5yZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_duplicates_in_attributions(df, max_timedelta):\n",
        "    sorted = df.sort_values(['user_id', 'revenue'])\n",
        "    \n",
        "    # Get values of the previous row\n",
        "    sorted['last_ts'] = sorted['ts'].shift(1)\n",
        "    sorted['last_user_id'] = sorted['user_id'].shift(1)\n",
        "    sorted['last_revenue'] = sorted['revenue'].shift(1)\n",
        "    \n",
        "    # Remove rows if the previous row has the same revenue and user id and the ts are less than max_timedelta apart\n",
        "    filtered = sorted[\n",
        "        (sorted['user_id'] != sorted['last_user_id']) |\n",
        "        (sorted['revenue'] != sorted['last_revenue']) |\n",
        "        ((pd.to_datetime(sorted['ts']) - pd.to_datetime(sorted['last_ts'])) > max_timedelta)]\n",
        "    \n",
        "    return filtered[['user_id', 'revenue_eur', 'ts', 'partner_event', 'ab_test_group']]\n",
        "\n",
        "if use_deduplication:\n",
        "  attributions_df = drop_duplicates_in_attributions(attributions_df, pd.Timedelta('1 minute'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mFwmhMrJy0_z"
      },
      "source": [
        "### Remove invalid users with different groups in mark events\n",
        "\n",
        "Due to a race condition during marking we need to filter out users that are marked as *control* and *test*. In rare cases we see the same user on different servers in the same second, and unknowingly of each other marked him differently. Under certain circumstances this affected the storage of the user - instead of marking him as *test* and *control* he was marked only as one but the opposite group was stored and used for further events. We filter for this case in the `uplift` method directly. Both issues were fixed in the latest version of the remerge plattform but we need to filter old data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AX5RSfYo641",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# users that are in both groups due to racy bids are invalid\n",
        "# we need to filter them out\n",
        "groups_per_user = bids_df.groupby('user_id')['ab_test_group'].nunique()\n",
        "\n",
        "double_marked_users = groups_per_user[groups_per_user > 1].index.to_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdHAx_rIpWIx",
        "colab_type": "text"
      },
      "source": [
        "### Remove users with incorrect groups between marks and attributions stream\n",
        "\n",
        "Some users have conversion/attribution events with the incorrect group (opposing the mark group). Those users need to be removed from the test as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgH8nWvxpRyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_users(audience, date, marks, chunk_size=10 ** 6):\n",
        "    source = 'attributions'\n",
        "    now = datetime.now()\n",
        "\n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "    \n",
        "    # local cache\n",
        "    cache_dir = '{0}/{1}/{2}'.format(cache_folder, audience, source)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "    \n",
        "    filename = '{0}/{1}/{2}.csv.gz'.format(path(audience), source, date_str)\n",
        "\n",
        "\n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    fs.connect_timeout = 10  # defaults to 5\n",
        "    fs.read_timeout = 30  # defaults to 15 \n",
        "    \n",
        "    \n",
        "    # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "    s3_cache_filename = '{0}/{1}/{2}/{3}.users.parquet'.format(path(audience),\n",
        "                                                           source, cache_folder, date_str)\n",
        "\n",
        "\n",
        "    cache_filename = '{0}/{1}.users.parquet'.format(cache_dir, date_str)\n",
        "    \n",
        "    if os.path.exists(cache_filename):\n",
        "        print(now, '[users] loading from', cache_filename)\n",
        "        df = from_parquet(cache_filename)\n",
        "        df['user_id'] = df['user_id'].astype('int64')\n",
        "        df['valid'] = df['valid'].astype('bool')\n",
        "        return df\n",
        "        \n",
        "\n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    fs.connect_timeout = 10  # defaults to 5\n",
        "    fs.read_timeout = 30  # defaults to 15 \n",
        "\n",
        "    if fs.exists(path=s3_cache_filename):\n",
        "        print(now, '[users] loading from S3 cache', s3_cache_filename)\n",
        "        \n",
        "        # Download the file to local cache first to avoid timeouts during the load.\n",
        "        # This way, if they happen, restart will be using local copies first.\n",
        "        fs.get(s3_cache_filename, cache_filename)\n",
        "        \n",
        "        print(now, '[users] stored S3 cache file to local drive, loading', cache_filename)\n",
        "        \n",
        "        return from_parquet(cache_filename)\n",
        "\n",
        "\n",
        "    print(now, '[users] start loading CSV for', audience, source, date)\n",
        "\n",
        "    read_csv_kwargs = {'chunksize': chunk_size}\n",
        "    read_csv_kwargs['usecols'] = ['ts', 'user_id', 'ab_test_group']\n",
        "\n",
        "    df = pd.DataFrame(columns=['user_id','valid'])\n",
        "   \n",
        "    if not fs.exists(path=filename):\n",
        "       print(now, 'WARNING: no CSV file at for: ', audience, source, date, ', skipping the file: ', filename)\n",
        "       return df\n",
        "      \n",
        "    for chunk in pd.read_csv(filename, escapechar='\\\\', low_memory=False,\n",
        "                             **read_csv_kwargs):\n",
        "        # remove all events that do not have a group\n",
        "        chunk = chunk[chunk['ab_test_group'].isin(['test','control'])]\n",
        "        chunk = improve_types(chunk)\n",
        "        merged_df = merge(marks, chunk)\n",
        "        invalid = merged_df[merged_df.ab_test_group_x != merged_df.ab_test_group_y]\n",
        "\n",
        "        invalid = invalid[['user_id']].copy()\n",
        "        invalid['valid'] = False\n",
        "        \n",
        "        \n",
        "        df = pd.concat([invalid,df],\n",
        "                       ignore_index=True,\n",
        "                       verify_integrity=True)\n",
        "        \n",
        "        valid = chunk[['user_id']].copy()\n",
        "        valid['valid'] = True\n",
        "\n",
        "        df = pd.concat([df,valid], ignore_index=True, verify_integrity=True)\n",
        "        \n",
        "        df = df.drop_duplicates(['user_id'])\n",
        "       \n",
        "        \n",
        "    print(datetime.now(), '[users] finished loading CSV for', date.strftime('%d.%m.%Y'),\n",
        "          'took', datetime.now() - now)\n",
        "    \n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    to_parquet(df, cache_filename)\n",
        "\n",
        "    # write it to the S3 cache folder as well\n",
        "    print(datetime.now(), '[users] caching as parquet', s3_cache_filename)\n",
        "\n",
        "    to_parquet(df, s3_cache_filename)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Nw3qnrpSmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mark_df = marked(bids_df)\n",
        "users_df = pd.DataFrame()\n",
        "# set the types explicitly otherwise we end up with object\n",
        "users_df['user_id'] = pd.Series(dtype='int64')\n",
        "users_df['valid'] = pd.Series(dtype='bool')\n",
        "\n",
        "i = 0\n",
        "for audience in audiences:\n",
        "  for date in dates:\n",
        "    print(audience,date)   \n",
        "    f = load_users(audience, date, mark_df)\n",
        "    users_df = pd.concat([ f[f.valid == False], users_df  ], ignore_index=True, verify_integrity=True)\n",
        "    users_df = pd.concat([ users_df, f[f.valid == True] ], ignore_index=True, verify_integrity=True)\n",
        "    i = i + 1\n",
        "    if i % 5 == 0:\n",
        "       users_df = users_df.drop_duplicates(['user_id']) \n",
        "    \n",
        "    \n",
        "users_df = users_df.drop_duplicates(['user_id'])                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4B9TCLJ4HUX",
        "colab_type": "text"
      },
      "source": [
        "### Analyze activity bias impact\n",
        "Calculate the ratio of users which are present in the `marks` but not in `attributions`. This represents "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KklttzQvrMIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_in_attr = mark_df[~mark_df['user_id'].isin(users_df['user_id'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yy6cum-rvma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test and control group have a different inactivity behavior so we need to account for it\n",
        "test_inactivity_bias = not_in_attr[not_in_attr.ab_test_group == TEST]['user_id'].nunique() / mark_df[mark_df.ab_test_group == TEST]['user_id'].nunique()\n",
        "control_inactivity_bias = not_in_attr[not_in_attr.ab_test_group == CONTROL]['user_id'].nunique() / mark_df[mark_df.ab_test_group == CONTROL]['user_id'].nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "armi-3kmy1Ag"
      },
      "source": [
        "## Uplift Calculation\n",
        "\n",
        "We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). Afterwards we run a [chi squared test](https://en.wikipedia.org/wiki/Chi-squared_test) on the results to test for significance of the results, comparing conversion to per group uniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V1vKf_u5y1Ag",
        "colab": {}
      },
      "source": [
        "def uplift(bids_df, attributions_df, index_name, m_hypothesis=1, invalid_users=None, test_inactivity_bias=None, control_inactivity_bias=None):\n",
        "    # CORRECTION: remove invalid users if any\n",
        "    test_invalid_users = 0\n",
        "    control_invalid_users = 0\n",
        "    if invalid_users is not None:\n",
        "        #memorize how many invalid users we are removing per group\n",
        "        invalid_bids_index = bids_df['user_id'].isin(invalid_users)\n",
        "        invalid_bids = bids_df[invalid_bids_index]      \n",
        "        test_invalid_users = invalid_bids[invalid_bids['ab_test_group'] == TEST]['user_id'].nunique()\n",
        "        control_invalid_users = invalid_bids[invalid_bids['ab_test_group'] == CONTROL]['user_id'].nunique()\n",
        "\n",
        "        # remove invalid users from the main dfs\n",
        "        bids_df = bids_df[~invalid_bids_index]      \n",
        "        attributions_df = attributions_df[~attributions_df['user_id'].isin(invalid_users)]\n",
        "\n",
        "    \n",
        "    # filter for mark events\n",
        "    marks_df = marked(bids_df)\n",
        "        \n",
        "    # calculate group sizes\n",
        "    test_group_size = marks_df[marks_df['ab_test_group'] == TEST]['user_id'].nunique()\n",
        "    if test_group_size == 0:\n",
        "        print(\"WARNING: No users marked as test for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "    \n",
        "    control_group_size = marks_df[marks_df['ab_test_group'] == CONTROL]['user_id'].nunique()\n",
        "    if control_group_size == 0:\n",
        "        print(\"WARNING: No users marked as control for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "\n",
        "    # CORRECTION: compensate for inactivity bias since there are some unobserved invalid users\n",
        "    # reasoning: we can observed the invalid users only when they have any attribution activity\n",
        "    # hence we must compensate for invalid users which were inactive (=not seen in attribution stream)\n",
        "    if invalid_users is not None:\n",
        "        if test_inactivity_bias is not None:\n",
        "            test_missing_invalid_correction = 1 / (1 - test_inactivity_bias) - 1\n",
        "            test_group_size -= test_invalid_users * test_missing_invalid_correction\n",
        "        if control_inactivity_bias is not None:\n",
        "            control_missing_invalid_correction = 1 / (1 - control_inactivity_bias) - 1\n",
        "            control_group_size -= control_invalid_users * control_missing_invalid_correction\n",
        "    \n",
        "\n",
        "    # Dask based join, for later and bigger datasets\n",
        "    # marks_df = dd.from_pandas(marks_df, npartitions=10, sort=True)    \n",
        "    # attributions_df = dd.from_pandas(attributions_df, npartitions=20, sort=True)\n",
        "    # merged_df = dd.merge(attributions_df, marks_df, on='user_id')\n",
        "    # merged_df = merged_df[merged_df.ts_x > merged_df.ts_y]\n",
        "    # merged_df = merged_df.compute()\n",
        "        \n",
        "    # join marks and revenue events    \n",
        "    merged_df = merge(marks_df, attributions_df)\n",
        "    grouped_revenue = merged_df.groupby(by='ab_test_group_y')\n",
        "\n",
        "    # init all KPIs with 0s first:\n",
        "    test_revenue_micros = 0\n",
        "    test_conversions = 0\n",
        "    test_converters = 0\n",
        "\n",
        "    control_revenue_micros = 0\n",
        "    control_conversions = 0\n",
        "    control_converters = 0\n",
        "\n",
        "    # we might not have any events for a certain group in the time-period,\n",
        "    if TEST in grouped_revenue.groups:\n",
        "        test_revenue_df = grouped_revenue.get_group(TEST)\n",
        "        test_revenue_micros = test_revenue_df['revenue_eur'].sum()\n",
        "        # test_conversions = test_revenue_df['partner_event'].count()\n",
        "        # as we filtered by revenue event and dropped the column we can just use\n",
        "        test_conversions = test_revenue_df['user_id'].count()\n",
        "        test_converters = test_revenue_df['user_id'].nunique()\n",
        "\n",
        "    if CONTROL in grouped_revenue.groups:\n",
        "        control_revenue_df = grouped_revenue.get_group(CONTROL)\n",
        "        control_revenue_micros = control_revenue_df['revenue_eur'].sum()\n",
        "        # control_conversions = control_revenue_df['partner_event'].count()\n",
        "        # as we filtered by revenue event and dropped the column we can just use\n",
        "        control_conversions = control_revenue_df['user_id'].count()\n",
        "        control_converters = control_revenue_df['user_id'].nunique()\n",
        "\n",
        "\n",
        "    # calculate KPIs\n",
        "    test_revenue = test_revenue_micros / 10 ** 6\n",
        "    control_revenue = control_revenue_micros / 10 ** 6\n",
        "\n",
        "    ratio = float(test_group_size) / float(control_group_size)\n",
        "    scaled_control_conversions = float(control_conversions) * ratio\n",
        "    scaled_control_revenue_micros = float(control_revenue_micros) * ratio\n",
        "    incremental_conversions = test_conversions - scaled_control_conversions\n",
        "    incremental_revenue_micros = test_revenue_micros - scaled_control_revenue_micros\n",
        "    incremental_revenue = incremental_revenue_micros / 10 ** 6\n",
        "    incremental_converters = test_converters - control_converters * ratio\n",
        "\n",
        "    # calculate the ad spend        \n",
        "    ad_spend = calculate_ad_spend(bids_df)  \n",
        "\n",
        "    iroas = incremental_revenue / ad_spend\n",
        "    icpa = ad_spend / incremental_conversions\n",
        "    cost_per_incremental_converter = ad_spend / incremental_converters\n",
        "    \n",
        "    rev_per_conversion_test = 0\n",
        "    rev_per_conversion_control = 0\n",
        "    if test_conversions > 0:\n",
        "        rev_per_conversion_test = test_revenue / test_conversions\n",
        "    if control_conversions > 0:\n",
        "        rev_per_conversion_control = control_revenue / control_conversions\n",
        "\n",
        "    test_cvr = test_conversions / test_group_size\n",
        "    control_cvr = control_conversions / control_group_size\n",
        "\n",
        "    uplift = 0\n",
        "    if control_cvr > 0:\n",
        "        uplift = test_cvr / control_cvr - 1\n",
        "\n",
        "    # calculate statistical significance\n",
        "    control_successes, test_successes = control_conversions, test_conversions\n",
        "    if use_converters_for_significance or max(test_cvr, control_cvr) > 1.0:\n",
        "        control_successes, test_successes = control_converters, test_converters\n",
        "    chi_df = pd.DataFrame({\n",
        "        \"conversions\": [control_successes, test_successes],\n",
        "        \"total\": [control_group_size, test_group_size]\n",
        "    }, index=['control', 'test'])\n",
        "    # CHI square calculation will fail with insufficient data\n",
        "    # Fallback to no significance\n",
        "    try:\n",
        "        chi, p, *_ = scipy.stats.chi2_contingency(\n",
        "            pd.concat([chi_df.total - chi_df.conversions, chi_df.conversions], axis=1), correction=False)\n",
        "    except:\n",
        "        chi, p = 0, 1.0\n",
        "        \n",
        "    # bonferroni correction with equal weights - if we have multiple hypothesis:\n",
        "    # https://en.wikipedia.org/wiki/Bonferroni_correction\n",
        "    significant = p < 0.05/m_hypothesis\n",
        "\n",
        "    dataframe_dict = {\n",
        "        \"ad spend\": ad_spend,\n",
        "        \"total revenue\": test_revenue + control_revenue,\n",
        "        \"test group size\": test_group_size,\n",
        "        'test invalid users': test_invalid_users,\n",
        "        \"test conversions\": test_conversions,\n",
        "        \"test converters\": test_converters,\n",
        "        \"test revenue\": test_revenue,\n",
        "        \"control group size\": control_group_size,\n",
        "        'control invalid users': control_invalid_users,\n",
        "        \"control conversions\": control_conversions,\n",
        "        \"control_converters\": control_converters,\n",
        "        \"control revenue\": control_revenue,\n",
        "        \"ratio test/control\": ratio,\n",
        "        \"control conversions (scaled)\": scaled_control_conversions,\n",
        "        \"control revenue (scaled)\": scaled_control_revenue_micros / 10 ** 6,\n",
        "        \"incremental conversions\": incremental_conversions,\n",
        "        \"incremental converters\": incremental_converters,\n",
        "        \"incremental revenue\": incremental_revenue,\n",
        "        \"rev/conversions test\": rev_per_conversion_test,\n",
        "        \"rev/conversions control\": rev_per_conversion_control,\n",
        "        \"test CVR\": test_cvr,\n",
        "        \"control CVR\": control_cvr,\n",
        "        \"CVR Uplift\": uplift,\n",
        "        \"iROAS\": iroas,\n",
        "        \"cost per incr. converter\": cost_per_incremental_converter,\n",
        "        \"iCPA\": icpa,\n",
        "        \"chi^2\": chi,\n",
        "        \"p-value\": p,\n",
        "        \"significant\": significant\n",
        "    }\n",
        "\n",
        "    # show results as a dataframe\n",
        "    return pd.DataFrame(\n",
        "        dataframe_dict,\n",
        "        index=[index_name],\n",
        "    ).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKdBRvkxL8Aa",
        "colab_type": "text"
      },
      "source": [
        "### Calculate and display uplift report for the data set as a whole\n",
        "\n",
        "This takes the whole data set and calculates uplift KPIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgTonVcVS5Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uplift_report(bids_df, attributions_df, groups, per_campaign_results, invalid_users=None, test_inactivity_bias=None, control_inactivity_bias=None):\n",
        "    # calculate the total result:\n",
        "    report_df = uplift(bids_df, attributions_df, \"total\",1, invalid_users, test_inactivity_bias, control_inactivity_bias)\n",
        "\n",
        "    # if there are groups filter the events against the per campaign groups and generate report\n",
        "    if len(groups) > 0:\n",
        "        for name, campaigns in groups.items():\n",
        "            group_bids_df = bids_df[bids_df.campaign_id.isin(campaigns)]\n",
        "            report_df[name] = uplift(group_bids_df, attributions_df, name, len(groups), invalid_users, test_inactivity_bias, control_inactivity_bias)\n",
        "            \n",
        "    if per_campaign_results:\n",
        "        campaigns = bids_df['campaign_name'].unique()\n",
        "        for campaign in campaigns:\n",
        "            name = \"c_{0}\".format(campaign)\n",
        "            campaign_bids_df = bids_df[bids_df.campaign_name == campaign]\n",
        "            report_df[name] = uplift(campaign_bids_df, attributions_df, name, len(campaigns), invalid_users, test_inactivity_bias, control_inactivity_bias)\n",
        "    return report_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjX4uu6tTpK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uncorrected_report = uplift_report(bids_df, attributions_df, groups, per_campaign_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LCBnn53UKda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "invalid_users = users_df[users_df.valid==False]['user_id']\n",
        "corrected_report = uplift_report(bids_df, attributions_df, groups, per_campaign_results, invalid_users)\n",
        "corrected_with_inactivity_bias_report = uplift_report(bids_df, attributions_df, groups, per_campaign_results, invalid_users, test_inactivity_bias, control_inactivity_bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdFSmL3u8Pe4",
        "colab_type": "text"
      },
      "source": [
        "## Uplift Results\n",
        "\n",
        "You can configure the ouput by using variables in the 'Configuration' section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWQXKYXB8YO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set formatting options\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh2jhbyEacSy",
        "colab_type": "text"
      },
      "source": [
        "### Uncorrected report (previous version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2UZOvmlaXqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(uncorrected_report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW_5xgWwakiy",
        "colab_type": "text"
      },
      "source": [
        "### Without invalid users\n",
        "Without invalid users with different groups in marks + attributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xy_j_oap8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#display(corrected_report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HcJNeVyw1Y",
        "colab_type": "text"
      },
      "source": [
        "### With inactivity bias correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k3xn8Xny8K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(corrected_with_inactivity_bias_report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8TRlf9pExwF",
        "colab_type": "text"
      },
      "source": [
        "### Results comparison - side by side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePtWFHl76b57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(\n",
        "    {\n",
        "        'uncorrected total': uncorrected_report['total'],\n",
        "        'without invalid users': corrected_report['total'],\n",
        "        'inactivity bias corrected': corrected_with_inactivity_bias_report['total'],\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "date=dates.strftime('%Y-%m-%d')\n",
        "start=str(date[0])\n",
        "end=str(date[-1])\n",
        "period='{}/{}'.format(start,end)\n",
        "\n",
        "invalid_users_count = invalid_users.count()\n",
        "\n",
        "activity_df = pd.DataFrame([\n",
        "    [customer, customer, customer],\n",
        "    [period, period, period],\n",
        "    [0,0,test_inactivity_bias],\n",
        "    [0,0,control_inactivity_bias],\n",
        "    [0,invalid_users_count,invalid_users_count]\n",
        "    ],  \n",
        "    columns=['uncorrected total', 'without invalid users', 'inactivity bias corrected'],\n",
        "    index=['customer',\n",
        "           'start/end',\n",
        "           'test inactivity bias',\n",
        "           'control inactivity bias',\n",
        "           'invalid users removed'])\n",
        "\n",
        "combined_results = activity_df.append(results)\n",
        "display(combined_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VA_k2BobaZS",
        "colab_type": "text"
      },
      "source": [
        "### CSV Export - combined reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-T6quwwbObO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "name='{}_{}-{}{}'.format(customer,start,end,'.csv')\n",
        "combined_results.transpose().to_csv(name) \n",
        "files.download(name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}