{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Uplift report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remerge/uplift-report/blob/master/uplift_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5mOAgeTNy0-o"
      },
      "cell_type": "markdown",
      "source": [
        "# remerge uplift report\n",
        "\n",
        "This notebook allows you to validate remerge provided uplift reporting numbers. To do so it downloads and analyses exported campaign and event data from S3. The campaign data contains all users that remerge marked to be part of an uplift test, the A/B group assignment, the timestamp of marking, conversion events (click, app open or similar) and their cost. The event data reflects the app event stream and includes events, their timestamp and revenue (if any). We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). \n",
        "\n",
        "**Hint**: This notebook can be run in any Jupyter instance with enough space/memory, as a [Google Colab notebook](#Google-Colab-version) or as a standalone Python script.\n",
        "\n",
        "### Notebook configuration\n",
        "\n",
        "For this notebook to work properly several variables in the [Configuration](#Configuration) section need to be be set: `customer`, `audience`, `\n",
        "revenue_event`, `dates` and the AWS credentials. All of these will be provided by your remerge account manager. \n",
        "\n",
        "\n",
        "### Verification\n",
        "\n",
        "To verify that the group split is random and has no bias, user events / attributes before the campaign start can be compared and checked for an equal distribution in test and control group. For example the user age distribution, the user activity distribution or the average spend per user  should be the same in both groups pre campaign.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OYbsRBPTaCZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Colab support\n",
        "\n",
        "This notebook can be run inside Google Colab. Due to size limitations it cointains several optimizations like removing unused fields from the input files and caching files. Furthermore it installs missing dependencies and restarts the kernel. **Because pandas is upgraded the kernel needs to be restarted once per fresh instance. Just run the cell again after restart** "
      ]
    },
    {
      "metadata": {
        "id": "QoJAOpL0aEIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d5a71707-2edd-4385-d47b-45873d807b9f"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  !pip install pyarrow\n",
        "  import pandas as pdt\n",
        "  if pdt.__version__ != '0.23.4':\n",
        "    # upgrading pandas requires a restart of the kernel\n",
        "    # (we need an up to date pandas because we write to S3 for caching)\n",
        "    # we kill it and let it auto restart (only needed once per fresh instance)\n",
        "    !pip install pandas==0.23.4\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (0.12.1)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kyYz6TCny0-q"
      },
      "cell_type": "markdown",
      "source": [
        "## Import needed packages\n",
        "\n",
        "This notebook/script needs pandas and scipy for analysis and boto to access data store on S3.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GWizAQT3y0-r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import scipy\n",
        "import scipy.stats \n",
        "import s3fs\n",
        "from IPython.display import display # so we can run this as script as well\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8u6Q76fCy0-u"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Set the customer name, audience and access credentials for the S3 bucket and path. Furthermore the event for which we want to evaluate the uplift needs to be set `revenue_event`."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aRd9FvoUy0-v",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# configure path and revenue event \n",
        "customer = ''\n",
        "audiences = ['']\n",
        "revenue_event = ''\n",
        "\n",
        "# date range for the report\n",
        "dates = pd.date_range(start='2019-02-14',end='2019-02-27')\n",
        "\n",
        "# AWS credentials\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ''\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7XnJeThPiSye"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper\n",
        "Define a few helper functions to load and cache data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bdeQztnqy0--",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def path(audience):\n",
        "  return \"s3://remerge-customers/{0}/uplift_data/{1}\".format(customer,audience)\n",
        "\n",
        "# helper to remove a few things we load if we run in Google Colab\n",
        "def limit_df(df,source):\n",
        "    if not IN_COLAB:\n",
        "      return df\n",
        "    if source != 'attributions':\n",
        "      return df\n",
        "    # we drop a few things so we fit into the Colab memory limit\n",
        "    df.drop(['partner','revenue','revenue_currency'], axis=1)\n",
        "    df = df[df.partner_event == revenue_event]\n",
        "    gc.collect()\n",
        "    return df \n",
        "\n",
        "# helper to download CSV files, convert to DF and print time needed\n",
        "# caches files locally and on S3 to be reused\n",
        "def read_csv(audience, source, date):\n",
        "    now = datetime.now()\n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "    filename = '{0}/{1}/{2}.csv.gz'.format(path(audience), source, date_str)\n",
        "    # local cache\n",
        "    cache_dir = 'cache/{0}/{1}'.format(audience, source)\n",
        "    cache_filename = '{0}/{1}.parquet'.format(cache_dir, date_str)\n",
        "    if os.path.exists(cache_filename):\n",
        "        print(now, 'loading from', cache_filename)\n",
        "        df = pd.read_parquet(cache_filename, engine='pyarrow')\n",
        "        df = limit_df(df,source)\n",
        "        return df\n",
        "    # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "    s3_cache_filename = '{0}/{1}/cache/{2}.parquet'.format(path(audience), source, date_str)\n",
        "    fs =s3fs. S3FileSystem(anon=False)\n",
        "    if fs.exists(path=s3_cache_filename):\n",
        "      print(now, 'loading from S3 cache', s3_cache_filename)\n",
        "      return pd.read_parquet(s3_cache_filename, engine='pyarrow')\n",
        "    print(now, 'start loading CSV for', date)\n",
        "    df = pd.read_csv(filename, escapechar='\\\\')\n",
        "    df = limit_df(df,source)\n",
        "    print(datetime.now(), 'finished loading CSV for', date.strftime('%d.%m.%Y'), 'took', datetime.now()-now)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "    df.to_parquet(cache_filename, engine='pyarrow')\n",
        "    # write it to the S3 cache folder as well\n",
        "    print(datetime.now(), 'caching as parquet', s3_cache_filename)\n",
        "    df.to_parquet(s3_cache_filename, engine='pyarrow')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eSixTLyiy0_A"
      },
      "cell_type": "markdown",
      "source": [
        "## Load CSV data from S3\n",
        "\n",
        "Load mark, spend and event data from S3. \n",
        "\n",
        "### IMPORTANT\n",
        "\n",
        "**The event data is usually quite large (several GB) so this operation might take several minutes or hours to complete, depending on the size and connection.**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PjWaWZS-y0_B",
        "outputId": "0d171fbd-d2b9-432c-99ba-df9a97c11f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "bid_df = pd.concat([read_csv(audience,'marks_and_spend',date) for audience in audiences for date in dates], ignore_index = True, verify_integrity=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-11 11:38:37.455038 start loading CSV for 2019-02-14 00:00:00\n",
            "2019-03-11 11:38:37.729656 finished loading CSV for 14.02.2019 took 0:00:00.274650\n",
            "2019-03-11 11:38:37.755021 start loading CSV for 2019-02-15 00:00:00\n",
            "2019-03-11 11:38:37.938140 finished loading CSV for 15.02.2019 took 0:00:00.183151\n",
            "2019-03-11 11:38:37.972273 start loading CSV for 2019-02-16 00:00:00\n",
            "2019-03-11 11:38:38.140322 finished loading CSV for 16.02.2019 took 0:00:00.168088\n",
            "2019-03-11 11:38:38.177744 start loading CSV for 2019-02-17 00:00:00\n",
            "2019-03-11 11:38:38.354706 finished loading CSV for 17.02.2019 took 0:00:00.176996\n",
            "2019-03-11 11:38:38.388169 start loading CSV for 2019-02-18 00:00:00\n",
            "2019-03-11 11:38:38.613885 finished loading CSV for 18.02.2019 took 0:00:00.225749\n",
            "2019-03-11 11:38:38.646317 start loading CSV for 2019-02-19 00:00:00\n",
            "2019-03-11 11:38:39.191846 finished loading CSV for 19.02.2019 took 0:00:00.545579\n",
            "2019-03-11 11:38:39.361665 start loading CSV for 2019-02-20 00:00:00\n",
            "2019-03-11 11:38:39.670646 finished loading CSV for 20.02.2019 took 0:00:00.309029\n",
            "2019-03-11 11:38:39.761113 start loading CSV for 2019-02-21 00:00:00\n",
            "2019-03-11 11:38:40.007912 finished loading CSV for 21.02.2019 took 0:00:00.246846\n",
            "2019-03-11 11:38:40.082241 start loading CSV for 2019-02-22 00:00:00\n",
            "2019-03-11 11:38:40.315508 finished loading CSV for 22.02.2019 took 0:00:00.233314\n",
            "2019-03-11 11:38:40.381292 start loading CSV for 2019-02-23 00:00:00\n",
            "2019-03-11 11:38:40.631171 finished loading CSV for 23.02.2019 took 0:00:00.249923\n",
            "2019-03-11 11:38:40.696411 start loading CSV for 2019-02-24 00:00:00\n",
            "2019-03-11 11:38:40.945398 finished loading CSV for 24.02.2019 took 0:00:00.249020\n",
            "2019-03-11 11:38:41.005930 start loading CSV for 2019-02-25 00:00:00\n",
            "2019-03-11 11:38:41.226809 finished loading CSV for 25.02.2019 took 0:00:00.220916\n",
            "2019-03-11 11:38:41.290906 start loading CSV for 2019-02-26 00:00:00\n",
            "2019-03-11 11:38:41.532619 finished loading CSV for 26.02.2019 took 0:00:00.241758\n",
            "2019-03-11 11:38:41.591738 start loading CSV for 2019-02-27 00:00:00\n",
            "2019-03-11 11:38:41.820482 finished loading CSV for 27.02.2019 took 0:00:00.228788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zxjJ4gK3y0_E",
        "outputId": "8fbf1bbd-c98f-444d-c02c-ed9e29f48577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "attributions_df = pd.concat([read_csv(audience,'attributions',date) for audience in audiences for date in dates], ignore_index = True, verify_integrity=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-11 09:50:50.152831 loading from cache cache/attributions/20190214.parquet\n",
            "2019-03-11 09:50:50.401528 loading from cache cache/attributions/20190215.parquet\n",
            "2019-03-11 09:50:50.639826 start loading CSV for 2019-02-16 00:00:00\n",
            "2019-03-11 09:52:05.588125 finished loading CSV for 16.02.2019 took 0:01:14.948358\n",
            "2019-03-11 09:52:05.972775 start loading CSV for 2019-02-17 00:00:00\n",
            "2019-03-11 09:53:25.215945 finished loading CSV for 17.02.2019 took 0:01:19.243234\n",
            "2019-03-11 09:53:25.557478 start loading CSV for 2019-02-18 00:00:00\n",
            "2019-03-11 09:54:47.680872 finished loading CSV for 18.02.2019 took 0:01:22.123456\n",
            "2019-03-11 09:54:48.025925 start loading CSV for 2019-02-19 00:00:00\n",
            "2019-03-11 09:56:09.794334 finished loading CSV for 19.02.2019 took 0:01:21.768470\n",
            "2019-03-11 09:56:10.137239 start loading CSV for 2019-02-20 00:00:00\n",
            "2019-03-11 09:57:38.803291 finished loading CSV for 20.02.2019 took 0:01:28.666105\n",
            "2019-03-11 09:57:39.141374 start loading CSV for 2019-02-21 00:00:00\n",
            "2019-03-11 09:58:58.400953 finished loading CSV for 21.02.2019 took 0:01:19.259640\n",
            "2019-03-11 09:58:58.736458 start loading CSV for 2019-02-22 00:00:00\n",
            "2019-03-11 10:00:14.853129 finished loading CSV for 22.02.2019 took 0:01:16.116731\n",
            "2019-03-11 10:00:15.187980 start loading CSV for 2019-02-23 00:00:00\n",
            "2019-03-11 10:01:31.996916 finished loading CSV for 23.02.2019 took 0:01:16.808993\n",
            "2019-03-11 10:01:32.327022 start loading CSV for 2019-02-24 00:00:00\n",
            "2019-03-11 10:02:50.850595 finished loading CSV for 24.02.2019 took 0:01:18.523626\n",
            "2019-03-11 10:02:51.166020 start loading CSV for 2019-02-25 00:00:00\n",
            "2019-03-11 10:04:16.410439 finished loading CSV for 25.02.2019 took 0:01:25.244523\n",
            "2019-03-11 10:04:16.773567 start loading CSV for 2019-02-26 00:00:00\n",
            "2019-03-11 10:05:40.312000 finished loading CSV for 26.02.2019 took 0:01:23.538501\n",
            "2019-03-11 10:05:40.635890 start loading CSV for 2019-02-27 00:00:00\n",
            "2019-03-11 10:06:59.614769 finished loading CSV for 27.02.2019 took 0:01:18.978938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ec_qFUaVy0_I"
      },
      "cell_type": "markdown",
      "source": [
        "Print some statistics of the loaded data sets."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N0Ih6SSuy0_J",
        "outputId": "3dfac786-30c4-46f8-945b-45b4678fc684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "bid_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 535434 entries, 0 to 535433\n",
            "Data columns (total 9 columns):\n",
            "ts               535434 non-null object\n",
            "event_type       535434 non-null object\n",
            "ab_test_group    535434 non-null object\n",
            "user_id          535422 non-null object\n",
            "campaign_id      535434 non-null int64\n",
            "cost_currency    25657 non-null object\n",
            "cost             25657 non-null float64\n",
            "cost_eur         25657 non-null float64\n",
            "campaign_name    535434 non-null object\n",
            "dtypes: float64(2), int64(1), object(6)\n",
            "memory usage: 36.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EoU_cW07y0_M",
        "outputId": "41644b5c-d577-4084-ba50-07758cec1755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "attributions_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 771714 entries, 0 to 771713\n",
            "Data columns (total 10 columns):\n",
            "ts                  771714 non-null object\n",
            "user_id             771714 non-null object\n",
            "event_id            0 non-null float64\n",
            "partner             771714 non-null object\n",
            "partner_event       771714 non-null object\n",
            "revenue             768543 non-null float64\n",
            "revenue_currency    771714 non-null object\n",
            "revenue_eur         768543 non-null float64\n",
            "ab_test_group       38044 non-null object\n",
            "event_data          771714 non-null object\n",
            "dtypes: float64(3), object(7)\n",
            "memory usage: 58.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XinLEFZcy0_V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set formatting options\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mFwmhMrJy0_z"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove invalid users\n",
        "\n",
        "Due to a race condition during marking we need to filter out users that are marked as *control* and *test*. In rare cases we see the same user on different servers in the same second, and unknowingly of each other marked him differently. This was fixed in the latest version of the remerge plattform but we need to filter old data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5bnOSTKly0_1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# users that are in both groups due to racy bids are invalid\n",
        "# we need to filter them out\n",
        "groups_per_user = bid_df.groupby('user_id')['ab_test_group'].nunique()\n",
        "invalid_users = groups_per_user[groups_per_user > 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4XVQNpr9y0_7"
      },
      "cell_type": "markdown",
      "source": [
        "The `mark_df` dataframe will contain all mark events (without the invalid marks). It is then grouped by the assigend A/B test group."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a6XtI0Iqy0_8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mark_df = bid_df[bid_df.event_type == 'mark']\n",
        "mark_df = mark_df[~mark_df['user_id'].isin(invalid_users.index)]\n",
        "grouped = mark_df.groupby(by='ab_test_group')\n",
        "control_df = grouped.get_group('control')\n",
        "test_df = grouped.get_group('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IPPvDwWIy1AC"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate the cost of advertising. Remerge tracks monetary values in micro currency units. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZEISdQDny1AC",
        "outputId": "7dcd92a1-043e-435d-be26-eb12540ea91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ad_spend_micros = bid_df[bid_df.event_type == 'buying_conversion']['cost_eur'].sum()\n",
        "ad_spend = ad_spend_micros / 10**6\n",
        "ad_spend"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6837.308504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EM9SbRf2y1AG"
      },
      "cell_type": "markdown",
      "source": [
        "Create a dataframe that contains all relevant revenue events."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ONjCFbzIy1AH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "revenue_df = attributions_df[pd.notnull(attributions_df['revenue_eur'])]\n",
        "revenue_df = revenue_df[revenue_df.partner_event == revenue_event]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8oPJNEFwy1AT"
      },
      "cell_type": "markdown",
      "source": [
        "Remerge marks users per campaign. This analysis looks at the per audience uplift, for that reason we drop duplicate marks for users that were marked by multiple campaigns. If a user was marked once for an audience he will have the same group allocation for consecutive marks unless manually reset on audience level.  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R1kAucoJy1AU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted_mark_df = mark_df.sort_values('ts')\n",
        "depuplicated_mark_df = sorted_mark_df.drop_duplicates(['user_id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ph6eaF4Ny1Ad"
      },
      "cell_type": "markdown",
      "source": [
        "Join the marked users with the revenue events and excluded any revenue event that happend before the user was marked."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3mReofc4y1Ad",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(revenue_df, depuplicated_mark_df, on='user_id')\n",
        "merged_df = merged_df[merged_df.ts_x > merged_df.ts_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "armi-3kmy1Ag"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate uplift kpis\n",
        "\n",
        "We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). Afterwards we run a [chi squared test](https://en.wikipedia.org/wiki/Chi-squared_test) on the results to test for significance of the results, comparing conversion to per group uniques."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V1vKf_u5y1Ag",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_revenue = merged_df.groupby(by='ab_test_group_y')\n",
        "test_group_size = test_df['user_id'].nunique()\n",
        "test_revenue_micros = grouped_revenue.get_group('test')['revenue_eur'].sum()\n",
        "test_revenue = test_revenue_micros / 10**6\n",
        "control_group_size = control_df['user_id'].nunique()\n",
        "control_revenue_micros = grouped_revenue.get_group('control')['revenue_eur'].sum()\n",
        "control_revenue = control_revenue_micros / 10**6\n",
        "test_conversions = grouped_revenue.get_group('test')['revenue_eur'].count()\n",
        "control_conversion = grouped_revenue.get_group('control')['revenue_eur'].count()\n",
        "ratio = float(test_group_size) / float(control_group_size)\n",
        "scaled_control_conversions = float(control_conversion) * ratio\n",
        "scaled_control_revenue_micros = float(control_revenue_micros) * ratio\n",
        "incremental_conversions = test_conversions - scaled_control_conversions\n",
        "incremental_revenue_micros = test_revenue_micros - scaled_control_revenue_micros\n",
        "incremental_revenue = incremental_revenue_micros / 10**6\n",
        "iroas = incremental_revenue / ad_spend\n",
        "chi_df = pd.DataFrame({\n",
        "    \"conversions\": [control_conversion, test_conversions],\n",
        "    \"total\": [control_group_size, test_group_size]\n",
        "    }, index=['control', 'test'])\n",
        "\n",
        "chi,p,*_ = scipy.stats.chi2_contingency(pd.concat([chi_df.total - chi_df.conversions, chi_df.conversions], axis=1), correction=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2y91jjPVy1Ai",
        "outputId": "3cba4301-8bc7-498b-d4a8-401ed5f38462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "cell_type": "code",
      "source": [
        "# show results as a dataframe\n",
        "# if you use Python 3.6+ and pandas 0.23+ columns is not needed as\n",
        "# the dict will keep its order\n",
        "# (older verison will sort this by name) \n",
        "\n",
        "result_df = pd.DataFrame({\n",
        "    \"ad spend\": ad_spend,\n",
        "    \"total revenue\": test_revenue + control_revenue,\n",
        "    \"test group size\": test_group_size,\n",
        "    \"test conversions\": test_conversions,\n",
        "    \"test revenue\": test_revenue,\n",
        "    \"size control group\": control_group_size,\n",
        "    \"control conversion\": control_conversion,\n",
        "    \"control revenue\": control_revenue,\n",
        "    \"ratio test/control\": ratio,\n",
        "    \"control conversions (scaled)\": scaled_control_conversions,\n",
        "    \"control revenue (scaled)\": scaled_control_revenue_micros / 10**6,\n",
        "    \"incremental conversions\": incremental_conversions,\n",
        "    \"incremental revenue\": incremental_revenue,\n",
        "    \"rev/conversions test\":test_revenue / test_conversions,\n",
        "    \"rev/conversions control\": control_revenue / control_conversion,\n",
        "    \"iROAS\": iroas,\n",
        "    \"chi^2\":chi,\n",
        "    \"p-value\":p,\n",
        "    \"significant\":p<0.05},index=[\"value\"], \n",
        "    columns=[\"ad spend\",\"total revenue\", \"test group size\",\"test conversions\",\n",
        "             \"test revenue\",\"size control group\",\"control conversion\",\n",
        "             \"control revenue\",\"ratio test/control\",\"control conversions (scaled)\",\n",
        "             \"control revenue (scaled)\",\"incremental conversions\",\n",
        "             \"incremental revenue\",\"rev/conversions test\",\n",
        "             \"rev/conversions control\",\"iROAS\",\"chi^2\",\"p-value\",\"significant\"]\n",
        "  ).transpose()\n",
        "\n",
        "\n",
        "display(result_df)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ad spend</th>\n",
              "      <td>6837.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total revenue</th>\n",
              "      <td>3168493.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test group size</th>\n",
              "      <td>332442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test conversions</th>\n",
              "      <td>18468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test revenue</th>\n",
              "      <td>2549640.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size control group</th>\n",
              "      <td>81292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversion</th>\n",
              "      <td>4402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue</th>\n",
              "      <td>618852.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ratio test/control</th>\n",
              "      <td>4.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversions (scaled)</th>\n",
              "      <td>18001.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue (scaled)</th>\n",
              "      <td>2530786.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental conversions</th>\n",
              "      <td>466.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental revenue</th>\n",
              "      <td>18854.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions test</th>\n",
              "      <td>138.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions control</th>\n",
              "      <td>140.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iROAS</th>\n",
              "      <td>2.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chi^2</th>\n",
              "      <td>2.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p-value</th>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>significant</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  value\n",
              "ad spend                        6837.31\n",
              "total revenue                3168493.83\n",
              "test group size                  332442\n",
              "test conversions                  18468\n",
              "test revenue                 2549640.92\n",
              "size control group                81292\n",
              "control conversion                 4402\n",
              "control revenue               618852.91\n",
              "ratio test/control                 4.09\n",
              "control conversions (scaled)   18001.89\n",
              "control revenue (scaled)     2530786.54\n",
              "incremental conversions          466.11\n",
              "incremental revenue            18854.37\n",
              "rev/conversions test             138.06\n",
              "rev/conversions control          140.58\n",
              "iROAS                              2.76\n",
              "chi^2                              2.46\n",
              "p-value                            0.12\n",
              "significant                       False"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}